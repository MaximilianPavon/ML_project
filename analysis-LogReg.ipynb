{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(open(\"data/train_data.csv\", \"rb\"), delimiter=\",\")\n",
    "train_labels = np.loadtxt(open(\"data/train_labels.csv\", \"rb\"), delimiter=\",\")\n",
    "test_data = np.loadtxt(open(\"data/test_data.csv\", \"rb\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_data_stand = scaler.fit_transform(train_data)\n",
    "test_data_stand = scaler.fit_transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogReg = LogisticRegression(multi_class='ovr', penalty='l1',tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.1,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg.fit(train_data_stand, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.1,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74329589731835888"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogReg.score(train_data_stand,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = LogReg.predict(test_data_stand)\n",
    "pred_log = LogReg.predict_log_proba(test_data_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.make_submission(pred, 'LogReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(alpha=0.001, hidden_layer_sizes = (250,200,180, 150, 120), max_iter = 50000, activation = 'tanh', verbose = 'True', solver='adam', learning_rate='adaptive', early_stopping=True, validation_fraction=0.25, tol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(250, 200, 180, 150, 120),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=50000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.25, verbose='True', warm_start=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.50697507\n",
      "Validation score: 0.614115\n",
      "Iteration 2, loss = 1.06804809\n",
      "Validation score: 0.611366\n",
      "Iteration 3, loss = 0.91943646\n",
      "Validation score: 0.632447\n",
      "Iteration 4, loss = 0.81612972\n",
      "Validation score: 0.624198\n",
      "Iteration 5, loss = 0.74084340\n",
      "Validation score: 0.626031\n",
      "Iteration 6, loss = 0.66069909\n",
      "Validation score: 0.609533\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(250, 200, 180, 150, 120),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=50000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.25, verbose='True', warm_start=False)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(train_data_stand, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(train_data_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71991748796699517"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(train_data_stand,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.round(nn.predict(train_data_stand)) == train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71991748796699517"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.round(y_pred) == train_labels) / len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3141"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.round(y_pred) == train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = nn.predict(test_data_stand)\n",
    "nn_pred_log = nn.predict_log_proba(test_data_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_stand, train_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2 = MLPClassifier(alpha=0.00000001, hidden_layer_sizes = (250,200,180, 150, 120), max_iter = 50000, activation = 'tanh', verbose = 'True', solver='adam', learning_rate='adaptive', early_stopping=True, validation_fraction=0.25, tol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.58634923\n",
      "Validation score: 0.570905\n",
      "Iteration 2, loss = 1.11169400\n",
      "Validation score: 0.596577\n",
      "Iteration 3, loss = 0.97088135\n",
      "Validation score: 0.627139\n",
      "Iteration 4, loss = 0.85143679\n",
      "Validation score: 0.623472\n",
      "Iteration 5, loss = 0.76467398\n",
      "Validation score: 0.627139\n",
      "Iteration 6, loss = 0.68509233\n",
      "Validation score: 0.632029\n",
      "Iteration 7, loss = 0.59934908\n",
      "Validation score: 0.623472\n",
      "Iteration 8, loss = 0.51506763\n",
      "Validation score: 0.618582\n",
      "Iteration 9, loss = 0.44552502\n",
      "Validation score: 0.629584\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=1e-08, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(250, 200, 180, 150, 120),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=50000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.25, verbose='True', warm_start=False)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63886342804766272"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nn2.predict(X_val) == y_val)/ len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73321109328443734"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nn2.predict(train_data_stand) == train_labels)/ len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn3 = MLPClassifier(alpha=0.00000001, hidden_layer_sizes = (250,200,180, 150, 120), max_iter = 50000, activation = 'tanh', verbose = 'True', solver='adam', learning_rate='adaptive', early_stopping=True, validation_fraction=0.25, tol=1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.51672486\n",
      "Validation score: 0.620532\n",
      "Iteration 2, loss = 1.07540857\n",
      "Validation score: 0.653529\n",
      "Iteration 3, loss = 0.94060331\n",
      "Validation score: 0.635197\n",
      "Iteration 4, loss = 0.83544205\n",
      "Validation score: 0.656279\n",
      "Iteration 5, loss = 0.75492389\n",
      "Validation score: 0.653529\n",
      "Iteration 6, loss = 0.66541742\n",
      "Validation score: 0.648029\n",
      "Iteration 7, loss = 0.60343716\n",
      "Validation score: 0.640697\n",
      "Validation score did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', alpha=1e-08, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(250, 200, 180, 150, 120),\n",
       "       learning_rate='adaptive', learning_rate_init=0.001, max_iter=50000,\n",
       "       momentum=0.9, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.25, verbose='True', warm_start=False)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn3.fit(train_data_stand,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7403162961265185"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(nn3.predict(train_data_stand) == train_labels)/ len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nn3 = nn3.predict(test_data_stand)\n",
    "pred_nn3_log = nn3.predict_log_proba(test_data_stand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.make_submission(pred_nn3, 'NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.97894976,  0.99735779,  0.        ],\n",
       "       [ 0.99513243,  0.99043086,  0.98964872]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.28909878\n",
      "Iteration 2, loss = 2.70760139\n",
      "Iteration 3, loss = 2.25590931\n",
      "Iteration 4, loss = 1.94647931\n",
      "Iteration 5, loss = 1.77618663\n",
      "Iteration 6, loss = 1.58783244\n",
      "Iteration 7, loss = 1.36535129\n",
      "Iteration 8, loss = 1.17427783\n",
      "Iteration 9, loss = 1.01640419\n",
      "Iteration 10, loss = 0.82456931\n",
      "Iteration 11, loss = 0.66671243\n",
      "Iteration 12, loss = 0.56901756\n",
      "Iteration 13, loss = 0.48762433\n",
      "Iteration 14, loss = 0.36836856\n",
      "Iteration 15, loss = 0.32908717\n",
      "Iteration 16, loss = 0.35066334\n",
      "Iteration 17, loss = 0.31212233\n",
      "Iteration 18, loss = 0.23727481\n",
      "Iteration 19, loss = 0.17289545\n",
      "Iteration 20, loss = 0.14416776\n",
      "Iteration 21, loss = 0.13990831\n",
      "Iteration 22, loss = 0.11345623\n",
      "Iteration 23, loss = 0.11714762\n",
      "Iteration 24, loss = 0.12536829\n",
      "Iteration 25, loss = 0.11804160\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.50604924\n",
      "Iteration 2, loss = 2.53492636\n",
      "Iteration 3, loss = 2.24896132\n",
      "Iteration 4, loss = 1.94506459\n",
      "Iteration 5, loss = 1.75778020\n",
      "Iteration 6, loss = 1.61057058\n",
      "Iteration 7, loss = 1.36921708\n",
      "Iteration 8, loss = 1.23351891\n",
      "Iteration 9, loss = 1.01855889\n",
      "Iteration 10, loss = 0.83559510\n",
      "Iteration 11, loss = 0.71995598\n",
      "Iteration 12, loss = 0.60696399\n",
      "Iteration 13, loss = 0.52929326\n",
      "Iteration 14, loss = 0.41598002\n",
      "Iteration 15, loss = 0.34409501\n",
      "Iteration 16, loss = 0.29474110\n",
      "Iteration 17, loss = 0.25720085\n",
      "Iteration 18, loss = 0.20495527\n",
      "Iteration 19, loss = 0.20468209\n",
      "Iteration 20, loss = 0.14515865\n",
      "Iteration 21, loss = 0.10533038\n",
      "Iteration 22, loss = 0.07459692\n",
      "Iteration 23, loss = 0.05922377\n",
      "Iteration 24, loss = 0.05674882\n",
      "Iteration 25, loss = 0.05071525\n",
      "Iteration 26, loss = 0.04300899\n",
      "Iteration 27, loss = 0.03718195\n",
      "Iteration 28, loss = 0.02912092\n",
      "Iteration 29, loss = 0.02501044\n",
      "Iteration 30, loss = 0.02388753\n",
      "Iteration 31, loss = 0.02354764\n",
      "Iteration 32, loss = 0.02343753\n",
      "Iteration 33, loss = 0.01926740\n",
      "Iteration 34, loss = 0.01754963\n",
      "Iteration 35, loss = 0.01867606\n",
      "Iteration 36, loss = 0.02354810\n",
      "Iteration 37, loss = 0.02205999\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.32793074\n",
      "Iteration 2, loss = 1.61651694\n",
      "Iteration 3, loss = 1.42230361\n",
      "Iteration 4, loss = 1.27916892\n",
      "Iteration 5, loss = 1.11479025\n",
      "Iteration 6, loss = 0.96048220\n",
      "Iteration 7, loss = 0.80283891\n",
      "Iteration 8, loss = 0.75398384\n",
      "Iteration 9, loss = 0.63977762\n",
      "Iteration 10, loss = 0.49844561\n",
      "Iteration 11, loss = 0.37983340\n",
      "Iteration 12, loss = 0.31834357\n",
      "Iteration 13, loss = 0.26563429\n",
      "Iteration 14, loss = 0.20178510\n",
      "Iteration 15, loss = 0.14343944\n",
      "Iteration 16, loss = 0.14335613\n",
      "Iteration 17, loss = 0.11989106\n",
      "Iteration 18, loss = 0.08078782\n",
      "Iteration 19, loss = 0.06551411\n",
      "Iteration 20, loss = 0.06142074\n",
      "Iteration 21, loss = 0.05332117\n",
      "Iteration 22, loss = 0.04445045\n",
      "Iteration 23, loss = 0.03573425\n",
      "Iteration 24, loss = 0.03241385\n",
      "Iteration 25, loss = 0.03145881\n",
      "Iteration 26, loss = 0.02501392\n",
      "Iteration 27, loss = 0.02066463\n",
      "Iteration 28, loss = 0.01667071\n",
      "Iteration 29, loss = 0.01409415\n",
      "Iteration 30, loss = 0.01244788\n",
      "Iteration 31, loss = 0.01276237\n",
      "Iteration 32, loss = 0.01133498\n",
      "Iteration 33, loss = 0.01123853\n",
      "Iteration 34, loss = 0.01032827\n",
      "Iteration 35, loss = 0.00951073\n",
      "Iteration 36, loss = 0.00956859\n",
      "Iteration 37, loss = 0.00817510\n",
      "Iteration 38, loss = 0.00954258\n",
      "Iteration 39, loss = 0.00963344\n",
      "Iteration 40, loss = 0.00891805\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.06652439\n",
      "Iteration 2, loss = 2.10220771\n",
      "Iteration 3, loss = 1.84287584\n",
      "Iteration 4, loss = 1.70341944\n",
      "Iteration 5, loss = 1.46537961\n",
      "Iteration 6, loss = 1.35878166\n",
      "Iteration 7, loss = 1.18062234\n",
      "Iteration 8, loss = 0.96881071\n",
      "Iteration 9, loss = 0.82088511\n",
      "Iteration 10, loss = 0.73424445\n",
      "Iteration 11, loss = 0.59773088\n",
      "Iteration 12, loss = 0.45037489\n",
      "Iteration 13, loss = 0.36943045\n",
      "Iteration 14, loss = 0.26101473\n",
      "Iteration 15, loss = 0.22119057\n",
      "Iteration 16, loss = 0.16154310\n",
      "Iteration 17, loss = 0.13739829\n",
      "Iteration 18, loss = 0.11602000\n",
      "Iteration 19, loss = 0.08767423\n",
      "Iteration 20, loss = 0.10315004\n",
      "Iteration 21, loss = 0.07965961\n",
      "Iteration 22, loss = 0.05517138\n",
      "Iteration 23, loss = 0.04595744\n",
      "Iteration 24, loss = 0.04187021\n",
      "Iteration 25, loss = 0.04044766\n",
      "Iteration 26, loss = 0.03031296\n",
      "Iteration 27, loss = 0.02324410\n",
      "Iteration 28, loss = 0.02117841\n",
      "Iteration 29, loss = 0.01974255\n",
      "Iteration 30, loss = 0.01847082\n",
      "Iteration 31, loss = 0.02043053\n",
      "Iteration 32, loss = 0.02180508\n",
      "Iteration 33, loss = 0.02280465\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.32164971\n",
      "Iteration 2, loss = 0.03844302\n",
      "Iteration 3, loss = 0.01574302\n",
      "Iteration 4, loss = 0.01050392\n",
      "Iteration 5, loss = 0.00661998\n",
      "Iteration 6, loss = 0.00533537\n",
      "Iteration 7, loss = 0.00446892\n",
      "Iteration 8, loss = 0.00406914\n",
      "Iteration 9, loss = 0.00382039\n",
      "Iteration 10, loss = 0.00364881\n",
      "Iteration 11, loss = 0.00346096\n",
      "Iteration 12, loss = 0.00333959\n",
      "Iteration 13, loss = 0.00323266\n",
      "Iteration 14, loss = 0.00315407\n",
      "Iteration 15, loss = 0.00309971\n",
      "Iteration 16, loss = 0.00303266\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.92569156\n",
      "Iteration 2, loss = 0.53215391\n",
      "Iteration 3, loss = 0.46008306\n",
      "Iteration 4, loss = 0.40836779\n",
      "Iteration 5, loss = 0.36570585\n",
      "Iteration 6, loss = 0.33659468\n",
      "Iteration 7, loss = 0.30688168\n",
      "Iteration 8, loss = 0.24671095\n",
      "Iteration 9, loss = 0.19613537\n",
      "Iteration 10, loss = 0.15500708\n",
      "Iteration 11, loss = 0.11769936\n",
      "Iteration 12, loss = 0.09497628\n",
      "Iteration 13, loss = 0.07006172\n",
      "Iteration 14, loss = 0.05705447\n",
      "Iteration 15, loss = 0.05721168\n",
      "Iteration 16, loss = 0.04299396\n",
      "Iteration 17, loss = 0.02943365\n",
      "Iteration 18, loss = 0.02458552\n",
      "Iteration 19, loss = 0.01857750\n",
      "Iteration 20, loss = 0.01769957\n",
      "Iteration 21, loss = 0.01292513\n",
      "Iteration 22, loss = 0.01155230\n",
      "Iteration 23, loss = 0.01089712\n",
      "Iteration 24, loss = 0.00999805\n",
      "Iteration 25, loss = 0.00926663\n",
      "Iteration 26, loss = 0.00857272\n",
      "Iteration 27, loss = 0.00780121\n",
      "Iteration 28, loss = 0.00823165\n",
      "Iteration 29, loss = 0.00779337\n",
      "Iteration 30, loss = 0.00829502\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "train_sizes, train_scores, valid_scores = learning_curve(nn2, train_data_stand, train_labels, train_sizes=[0.66, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10f5dc828>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f205320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_scores, label='train score')\n",
    "plt.plot(valid_scores, label='valid score')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
